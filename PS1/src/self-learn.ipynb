{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import util\n",
    "\n",
    "from linear_model import LinearModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(LinearModel):\n",
    "    \"\"\"Logistic regression with Newton's Method as the solver.\n",
    "\n",
    "    Example usage:\n",
    "        > clf = LogisticRegression()\n",
    "        > clf.fit(x_train, y_train)\n",
    "        > clf.predict(x_eval)\n",
    "    \"\"\"\n",
    "    def fit(self, x, y, method='gradient'):\n",
    "        \"\"\"Fit model using either Gradient Ascent or Newton's Method.\n",
    "\n",
    "        Args:\n",
    "            x: Training inputs (m, n).\n",
    "            y: Training labels (m,).\n",
    "            method: Optimization method ('gradient' or 'newton').\n",
    "        \"\"\"\n",
    "        # *** START CODE HERE ***\n",
    "        m, n = x.shape\n",
    "        self.theta = np.zeros(n) if self.theta is None else self.theta\n",
    "\n",
    "        if method == 'gradient':\n",
    "            for _ in range(self.max_iter):\n",
    "                h = sigmoid(x @ self.theta)\n",
    "                gradient = x.T @ (y - h) / m\n",
    "                self.theta += self.step_size * gradient\n",
    "\n",
    "        elif method == 'newton':\n",
    "            for i in range(self.max_iter):\n",
    "                h = sigmoid(x @ self.theta)\n",
    "                gradient = x.T @ (y - h) / m\n",
    "                S = np.diag(h * (1 - h))  # (m, m)\n",
    "                H = (x.T @ S @ x) / m     # (n, n)\n",
    "\n",
    "                try:\n",
    "                    delta = np.linalg.solve(H, gradient)\n",
    "                except np.linalg.LinAlgError:\n",
    "                    print(\"Warning: Hessian not invertible.\")\n",
    "                    break\n",
    "\n",
    "                self.theta += delta\n",
    "\n",
    "                # convergence check: stop if delta is small\n",
    "                if np.linalg.norm(delta, ord=1) < self.eps:\n",
    "                    if self.verbose:\n",
    "                        print(f\"Converged at iteration {i}\")\n",
    "                    break\n",
    "        else:\n",
    "            raise ValueError(\"Method must be 'gradient' or 'newton'.\")\n",
    "        # *** END CODE HERE ***\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Make a prediction given new inputs x.\n",
    "\n",
    "        Args:\n",
    "            x: Inputs of shape (m, n).\n",
    "\n",
    "        Returns:\n",
    "            Outputs of shape (m,).\n",
    "        \"\"\"\n",
    "        # *** START CODE HERE ***\n",
    "        return (sigmoid(x @ self.theta) > 0.5).astype(int)\n",
    "        # *** END CODE HERE ***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/Users/musazenbilci/Desktop/mosesopposite/cs229-2018-autumn/problem-sets/PS1/data/ds1_train.csv'\n",
    "eval_path = '/Users/musazenbilci/Desktop/mosesopposite/cs229-2018-autumn/problem-sets/PS1/data/ds1_valid.csv'\n",
    "# def main(train_path, eval_path, pred_path):\n",
    "\"\"\"Problem 1(b): Logistic regression with Newton's Method.\n",
    "\n",
    "Args:\n",
    "    train_path: Path to CSV file containing dataset for training.\n",
    "    eval_path: Path to CSV file containing dataset for evaluation.\n",
    "    pred_path: Path to save predictions.\n",
    "\"\"\"\n",
    "x_train, y_train = util.load_dataset(train_path, add_intercept=True)\n",
    "\n",
    "x_eval, y_eval = util.load_dataset(eval_path, add_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1\n",
      " 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1.]\n",
      "-36.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f9/kqml6rjn5g5688g1y7fn01ch0000gn/T/ipykernel_30413/1253679003.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# *** START CODE HERE ***\n",
    "clf = LogisticRegression()\n",
    "clf.fit(x_train, y_train, method='gradient')\n",
    "prediction = clf.predict(x_eval)\n",
    "print(prediction)\n",
    "print(y_eval)\n",
    "print((prediction-y_eval).sum())\n",
    "# *** END CODE HERE ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import util\n",
    "\n",
    "from linear_model import LinearModel\n",
    "\n",
    "\n",
    "def main(train_path, eval_path, pred_path):\n",
    "    \"\"\"Problem 1(e): Gaussian discriminant analysis (GDA)\n",
    "\n",
    "    Args:\n",
    "        train_path: Path to CSV file containing dataset for training.\n",
    "        eval_path: Path to CSV file containing dataset for evaluation.\n",
    "        pred_path: Path to save predictions.\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    x_train, y_train = util.load_dataset(train_path, add_intercept=False)\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "\n",
    "class GDA(LinearModel):\n",
    "    \"\"\"Gaussian Discriminant Analysis.\n",
    "\n",
    "    Example usage:\n",
    "        > clf = GDA()\n",
    "        > clf.fit(x_train, y_train)\n",
    "        > clf.predict(x_eval)\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \"\"\"Fit a GDA model to training set given by x and y.\n",
    "\n",
    "        Args:\n",
    "            x: Training example inputs. Shape (m, n).\n",
    "            y: Training example labels. Shape (m,).\n",
    "\n",
    "        Returns:\n",
    "            theta: GDA model parameters.\n",
    "        \"\"\"\n",
    "        # *** START CODE HERE ***\n",
    "        m, n = x.shape\n",
    "        phi = np.mean(y)\n",
    "\n",
    "        mu0 = x[y == 0].mean(axis=0)\n",
    "        mu1 = x[y == 1].mean(axis=0)\n",
    "        self.mu = [mu0, mu1]\n",
    "\n",
    "        # Shared covariance\n",
    "        diffs = x - np.where(y[:, np.newaxis] == 1, mu1, mu0)\n",
    "        sigma = (diffs.T @ diffs) / m\n",
    "        self.sigma = sigma\n",
    "\n",
    "        # Inverse\n",
    "        sigma_inv = np.linalg.inv(sigma)\n",
    "\n",
    "        # Compute theta_1 and theta_0\n",
    "        theta_1 = sigma_inv @ (mu1 - mu0)\n",
    "        theta_0 = (\n",
    "            0.5 * (mu0 @ sigma_inv @ mu0 - mu1 @ sigma_inv @ mu1)\n",
    "            - np.log((1 - phi) / phi)\n",
    "        )\n",
    "\n",
    "        # Final theta with intercept\n",
    "        self.theta = np.zeros(n + 1)\n",
    "        self.theta[0] = theta_0\n",
    "        self.theta[1:] = theta_1\n",
    "        # *** END CODE HERE ***\n",
    "\n",
    "    def predict(self, x, method='theta'):\n",
    "        \"\"\"Make a prediction given new inputs x.\n",
    "\n",
    "        Args:\n",
    "            x: Inputs of shape (m, n).\n",
    "            method: theta | naive\n",
    "\n",
    "        Returns:\n",
    "            Outputs of shape (m,).\n",
    "        \"\"\"\n",
    "        # *** START CODE HERE ***\n",
    "        def px_y(x, sigma, mu):\n",
    "            \"\"\"\n",
    "            x: shape (m, n)\n",
    "            mu: shape (n,)\n",
    "            sigma: shape (n, n)\n",
    "            returns: shape (m,)\n",
    "            \"\"\"\n",
    "            m, n = x.shape\n",
    "            inv_sigma = np.linalg.inv(sigma)\n",
    "            det_sigma = np.linalg.det(sigma)\n",
    "            norm_const = 1.0 / ((2 * np.pi) ** (n / 2) * np.sqrt(det_sigma))\n",
    "\n",
    "            # Center x\n",
    "            centered = x - mu  # shape (m, n)\n",
    "            \n",
    "            # Compute Mahalanobis distance for each row: (x - mu)^T @ Sigma^-1 @ (x - mu)\n",
    "            # Using einsum for efficient row-wise dot product\n",
    "            exponent = -0.5 * np.einsum(\"ij,jk,ik->i\", centered, inv_sigma, centered)\n",
    "\n",
    "            return norm_const * np.exp(exponent)\n",
    "\n",
    "        if method == \"theta\":\n",
    "            x = util.add_intercept(x)\n",
    "            return 1 / (1 + np.exp(-x @ self.theta))\n",
    "        elif method == \"naive\":\n",
    "            px_y1 = px_y(x, self.sigma, self.mu[1])\n",
    "            px_y0 = px_y(x, self.sigma, self.mu[0])\n",
    "            p_y1 = 0.5  # (y_train == 1).sum() / m\n",
    "            p_y0 = 0.5  # (y_train == 0).sum() / m\n",
    "            prediction = (px_y1 * p_y1) / (px_y1*p_y1 + px_y0*p_y0)\n",
    "            return prediction\n",
    "        # *** END CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/Users/musazenbilci/Desktop/mosesopposite/cs229-2018-autumn/problem-sets/PS1/data/ds1_train.csv'\n",
    "x_train, y_train = util.load_dataset(train_path, add_intercept=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.91180854,  60.35961272],\n",
       "       [  3.77474554, 344.1492843 ],\n",
       "       [  2.61548828, 178.22208681],\n",
       "       [  2.01369376,  15.25947155],\n",
       "       [  2.75762504,  66.19417399],\n",
       "       [  0.97392246,  41.67766519],\n",
       "       [  3.06727469, 143.27558992],\n",
       "       [  2.76309408,  35.96990594],\n",
       "       [  2.7757715 ,  29.56907921],\n",
       "       [  2.10982995,  76.63672138]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96595743, 0.85103215])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GDA()\n",
    "clf.fit(x_train, y_train)\n",
    "clf.predict(x_train[440:442], method=\"naive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./phi_gda_estimation.png\">\n",
    "<br/>\n",
    "<img src=\"./mean_gda_estimation.png\">\n",
    "<br/>\n",
    "<img src=\"./sigma_gda_estiomation.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m = len(x_train)\n",
    "phi = y_train.sum() / m\n",
    "\n",
    "mu0 = np.sum(x_train[np.where(y_train == 0)], axis=0) / ((y_train == 0).sum())\n",
    "\n",
    "mu1 = np.sum(x_train[np.where(y_train == 1)], axis=0) / ((y_train == 1).sum())\n",
    "\n",
    "mu = np.array([mu0, mu1])\n",
    "\n",
    "diffs = x_train - mu[y_train.astype(int)]\n",
    "sigma = (diffs.T @ diffs) / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./gda_prediction.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def px_y(x, sigma, mu):\n",
    "    return np.exp((-1/2) * (x - mu).T @ np.linalg.inv(sigma) @ (x - mu)) / ((np.pi ** (2 / 2)) * np.sqrt(np.linalg.det(sigma))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = x_train[751]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[451]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9846356805435632)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "px_y1 = px_y(xx, sigma, mu[1])\n",
    "px_y0 = px_y(xx, sigma, mu[0])\n",
    "p_y1 = (y_train == 1).sum() / m\n",
    "p_y0 = (y_train == 0).sum() / m\n",
    "prediction = (px_y1 * p_y1) / (px_y1*p_y1 + px_y0*p_y0)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive-Unlabeled Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from p01b_logreg import LogisticRegression\n",
    "train_path = \"/Users/musazenbilci/Desktop/mosesopposite/cs229-2018-autumn/problem-sets/PS1/data/ds3_train.csv\"\n",
    "test_path = \"/Users/musazenbilci/Desktop/mosesopposite/cs229-2018-autumn/problem-sets/PS1/data/ds3_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_t, y_train_t = util.load_dataset(train_path, label_col='t')\n",
    "x_train_y, y_train_y = util.load_dataset(train_path, label_col='y')\n",
    "\n",
    "x_test_t, y_test_t = util.load_dataset(test_path, label_col='t')\n",
    "x_test_y, y_test_y = util.load_dataset(test_path, label_col='y')\n",
    "\n",
    "# Part (c): Train and test on true labels\n",
    "# Make sure to save outputs to pred_path_c\n",
    "\n",
    "# Use newton method with t labels\n",
    "clf = LogisticRegression()\n",
    "clf.fit(x=x_train_t, y=y_train_t, method='newton')\n",
    "clf.predict(x_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/Users/musazenbilci/Desktop/mosesopposite/cs229-2018-autumn/problem-sets/PS1/data/ds4_train.csv\"\n",
    "val_path = \"/Users/musazenbilci/Desktop/mosesopposite/cs229-2018-autumn/problem-sets/PS1/data/ds4_valid.csv\"\n",
    "x_train, y_train = util.load_dataset(train_path, add_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5,), (5,))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape, theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.], shape=(2500,))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = x_train @ theta\n",
    "np.exp(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits 1.0\n"
     ]
    }
   ],
   "source": [
    "x = x_train[i]\n",
    "y = y_train[i]\n",
    "logits = h(x @ theta)\n",
    "print(\"logits\", logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 5 is different from 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[86]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 5 is different from 1)"
     ]
    }
   ],
   "source": [
    "\n",
    "np.array([y]) @ x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5,), (1,), ())"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.T.shape, np.array([y]).shape, logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m gradient = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m x\n",
      "\u001b[31mValueError\u001b[39m: matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "gradient = x.T @ (y - logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta [0. 0. 0. 0. 0.]\n",
      "logits 1.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m logits = h(x @ theta)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mlogits\u001b[39m\u001b[33m\"\u001b[39m, logits)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m gradient = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mgradient\u001b[39m\u001b[33m\"\u001b[39m, gradient)\n\u001b[32m     15\u001b[39m theta += \u001b[32m0.000000001\u001b[39m * gradient\n",
      "\u001b[31mValueError\u001b[39m: matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "m, n = x_train.shape\n",
    "theta = np.zeros(n)\n",
    "def h(x):\n",
    "    return np.exp(x)\n",
    "\n",
    "epoch = 20\n",
    "for i in range(len(x_train)):\n",
    "    print(\"theta\", theta)\n",
    "    x = x_train[i]\n",
    "    y = y_train[i].item()\n",
    "    logits = h(x @ theta)\n",
    "    print(\"logits\", logits)\n",
    "    gradient = x.T @ (y - logits)\n",
    "    print(\"gradient\", gradient)\n",
    "    theta += 0.000000001 * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.        , 0.        , 1.        , 0.22499177, 0.88080895]),\n",
       " array([-23780.43032319,  -1529.38993991, -22251.04038328, -19894.26250972,\n",
       "        -20447.73674632]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[i], theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.0), np.float64(3705439.0))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 4\n",
    "h((x_train[i] @ theta)), y_train[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import util\n",
    "\n",
    "from linear_model import LinearModel\n",
    "\n",
    "\n",
    "def main(lr, train_path, eval_path, pred_path):\n",
    "    \"\"\"Problem 3(d): Poisson regression with gradient ascent.\n",
    "\n",
    "    Args:\n",
    "        lr: Learning rate for gradient ascent.\n",
    "        train_path: Path to CSV file containing dataset for training.\n",
    "        eval_path: Path to CSV file containing dataset for evaluation.\n",
    "        pred_path: Path to save predictions.\n",
    "    \"\"\"\n",
    "    # Load training set\n",
    "    x_train, y_train = util.load_dataset(train_path, add_intercept=True)\n",
    "    # The line below is the original one from Stanford. It does not include the intercept, but this should be added.\n",
    "    # x_train, y_train = util.load_dataset(train_path, add_intercept=False)\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    # Fit a Poisson Regression model\n",
    "    clf = PoissonRegression(step_size=0.1e-7, max_iter=200)\n",
    "    clf.fit(x_train, y_train)\n",
    "    # Run on the validation set, and use np.savetxt to save outputs to pred_path\n",
    "    x_val, y_val = util.load_dataset(val_path, add_intercept=True)\n",
    "    preds = clf.predict(x_val)\n",
    "    np.savetxt(pred_path, preds)\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "\n",
    "class PoissonRegression(LinearModel):\n",
    "    \"\"\"Poisson Regression.\n",
    "\n",
    "    Example usage:\n",
    "        > clf = PoissonRegression(step_size=lr)\n",
    "        > clf.fit(x_train, y_train)\n",
    "        > clf.predict(x_eval)\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \"\"\"Run gradient ascent to maximize likelihood for Poisson regression.\n",
    "\n",
    "        Args:\n",
    "            x: Training example inputs. Shape (m, n).\n",
    "            y: Training example labels. Shape (m,).\n",
    "        \"\"\"\n",
    "        # *** START CODE HERE ***\n",
    "        m, n = x.shape\n",
    "        self.theta = np.zeros(n)\n",
    "        def h(x):\n",
    "            return np.exp(x)\n",
    "\n",
    "        epoch = self.max_iter\n",
    "        for _ in range(epoch):\n",
    "            for i in range(len(x)):\n",
    "                logits = h(x[i] @ self.theta)\n",
    "                gradient = x[i].reshape((x[i].shape[0], 1)) @ np.array([y[i] - logits])\n",
    "                self.theta += self.step_size * gradient        \n",
    "\n",
    "        # *** END CODE HERE ***\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Make a prediction given inputs x.\n",
    "\n",
    "        Args:\n",
    "            x: Inputs of shape (m, n).\n",
    "\n",
    "        Returns:\n",
    "            Floating-point prediction for each input, shape (m,).\n",
    "        \"\"\"\n",
    "        # *** START CODE HERE ***\n",
    "        return h(x @ self.theta)\n",
    "        # *** END CODE HERE ***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/Users/musazenbilci/Desktop/mosesopposite/cs229-2018-autumn/problem-sets/PS1/data/ds4_train.csv\"\n",
    "val_path = \"/Users/musazenbilci/Desktop/mosesopposite/cs229-2018-autumn/problem-sets/PS1/data/ds4_valid.csv\"\n",
    "x_train, y_train = util.load_dataset(train_path, add_intercept=True)\n",
    "x_val, y_val = util.load_dataset(val_path, add_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-08"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12003128.81595367,  2830162.83790876, 20527360.15875288,\n",
       "        1606376.58911739,   478596.13620045,  5703974.36112889,\n",
       "         688888.38769774,   482388.0033151 ,  1907967.57361079,\n",
       "         113363.10624469,  4191656.6579308 ,   496518.39024542,\n",
       "        4190044.98514974,  1847738.35948409,   146387.87794461,\n",
       "        1168147.59610179,  1589893.80579298,  3150536.55234231,\n",
       "         407892.12436554,   123136.27307857,  2118015.69987693,\n",
       "         118232.56520093,  1183632.38984027,  4787284.55023211,\n",
       "         578867.05136255,  9574790.17495857,   409630.2898261 ,\n",
       "        1099321.39230494,  1744530.59255483,  1470552.07095193,\n",
       "         495153.31735969,  3468862.12607031,  5675893.27632901,\n",
       "       24961881.89916041,   465147.97110652, 14628494.51991394,\n",
       "         777519.35942133,  1288812.11509628,   454300.94405375,\n",
       "       10173635.83240312,  2684760.71138495,  7235160.74197726,\n",
       "         460142.8572708 ,   203729.76500251,  3452957.46968923,\n",
       "       10846931.97677287,  6562785.18271896,   344552.24117926,\n",
       "         995193.5938577 ,  1477755.98592195,  1527875.67333653,\n",
       "         227935.94987627,  4056664.73830638,   218538.1104834 ,\n",
       "        2127630.83687942,   352414.51659719,  5846801.97098769,\n",
       "        4574151.95045794,  2959834.76570644,   696581.37989207,\n",
       "       10982616.5881589 ,  4079784.10674617,  3985302.29060548,\n",
       "         318841.02939881,   345774.73745922,  2808004.13951393,\n",
       "         382984.54536957, 13826990.95631741,   657036.91921454,\n",
       "         905238.27699448,   267575.01045128,   252561.24750982,\n",
       "         344895.26926765,  1606879.73972091,   799751.77496879,\n",
       "        1960270.22331638,   854504.81559112, 10703637.81115488,\n",
       "        1947793.2130499 ,  3754014.82486077,  2219656.75849703,\n",
       "        7459533.66007189,  4392076.73363259,  8098618.23221287,\n",
       "        3009662.26589454,   310147.59846031,   432469.66326655,\n",
       "         932044.27759139,   443436.98845163,  1482489.47318563,\n",
       "         878881.4253815 ,  2237539.06902072,  2913382.64487691,\n",
       "         215780.69853356,   296951.53530637,   197024.20211899,\n",
       "        4727510.56129523,  1979782.44736556,  1152713.62749778,\n",
       "         543423.45663015,  1659632.54254891,   315554.95111822,\n",
       "         347191.91940044,  5755820.35316536,  6789380.45009522,\n",
       "        1080245.00916874,  2614855.15506364,  2730150.30111102,\n",
       "         260154.05257307,   645829.06348727,  6698042.49785165,\n",
       "         338892.65733014,  8308071.6440059 ,  3907264.24271209,\n",
       "        1555828.31226683,   760647.0010296 ,   532493.66350049,\n",
       "        2419626.77631305,  3747181.28118336,  1438855.93058848,\n",
       "        8532940.06123271,  1468005.46340476,  3824832.1547226 ,\n",
       "         170014.70984379,   723749.10209767,  3115400.44143058,\n",
       "         261735.3037309 ,  7605652.25589583,  1573923.39445761,\n",
       "        1604148.6222665 ,   757975.47156011,  9450225.90804014,\n",
       "       11602030.04347105,   181696.20010743,   153028.39148043,\n",
       "         723128.45579775, 11078993.73274875,   140602.07171969,\n",
       "         208442.71246212,   475683.41301587,  4299919.08976912,\n",
       "        2673859.59537273,   265309.6563024 ,   363471.156313  ,\n",
       "        5173364.94579154,   133737.0173773 ,   241321.17367066,\n",
       "        3473923.74690252,   190544.75201268,   156341.80238756,\n",
       "         370207.04824462,   134298.02639806,   240070.57885372,\n",
       "        1259736.50436498,  8207689.97560259,   741741.58700995,\n",
       "        9060518.58000249,  5472846.99393875,  3092219.49742716,\n",
       "       10358784.04429561,  1974337.71118895, 14608890.46373399,\n",
       "        5368785.4606967 ,   557405.35184052,  1031339.37154049,\n",
       "       18352231.45777999,    65979.79398327,  2302972.82149191,\n",
       "        1118793.60511247,  4044910.98602519,  1157075.74143281,\n",
       "        1377256.73522272,  3981677.39223902,  1278374.37683643,\n",
       "        5427880.95449849,  5489650.75863361, 15064707.68624887,\n",
       "         415901.95364289,   551050.06252224,  2270426.62612411,\n",
       "        2108815.42098893,   190237.26958965,  3337021.0905164 ,\n",
       "        1027671.88470694,  8820745.90529568,   370399.00482863,\n",
       "       33131159.64691876,   537188.21802925, 27694273.01407333,\n",
       "        1133211.3574189 ,  2084386.02461612,  3286414.2423017 ,\n",
       "         382884.54669562,  1150736.58251267,   697194.33564333,\n",
       "         510679.63196081,  9406782.88993014,   305525.2584942 ,\n",
       "        4349484.54082052,   313932.86759802,   613311.25839762,\n",
       "       20710919.5076373 ,  4361935.48783531,    73687.45807797,\n",
       "       15665475.78914369,   442887.04066903, 19684759.86136897,\n",
       "        5696690.37699119,  1225608.87945991, 15582962.05678294,\n",
       "        1820854.07938939,  1797373.22173091,  1043947.0811077 ,\n",
       "         472224.68771123,   178751.37400443,   815323.11616146,\n",
       "       15191179.36438794,  3139500.7385027 ,   195875.71148742,\n",
       "        1226517.270397  ,  8751517.51371147,  8846120.57036705,\n",
       "         568316.09738264,   334784.24040792,  9108458.90896115,\n",
       "         180603.10185435,  4497768.80138354,   272029.46125554,\n",
       "        2253158.53564617,  1806570.67451042,   111654.79172079,\n",
       "         132284.13571962,  2599751.59138686,  6820918.05043476,\n",
       "        5021535.47635437, 16358695.81428725,   360390.79283556,\n",
       "        1367889.91136869,   257963.47967907,   562410.1338593 ,\n",
       "         317306.82771913,   958282.22563887,  2181050.50247632,\n",
       "         654849.8070509 , 13311543.31636316,  5725371.92353773,\n",
       "       11601512.48170962,   142609.86942861,   401470.86595499,\n",
       "        3277422.62012993])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = PoissonRegression(step_size=0.1e-7, max_iter=200)\n",
    "clf.fit(x_train, y_train)\n",
    "clf.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_train\n",
    "y = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([203654.62911845, 194349.88006475, 190127.50206897, ...,\n",
       "       204259.99388508, 242291.16667128, 188213.7258473 ], shape=(2500,))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x @ theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0.]\n",
      "[8.81357768 3.52076832 5.29280936 5.79967485 6.99194722]\n"
     ]
    }
   ],
   "source": [
    "logits = h(x @ theta)\n",
    "logits\n",
    "gradient = x.T @ (y - logits)\n",
    "print(theta)\n",
    "theta += 0.000000001 * gradient\n",
    "print(theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        ],\n",
       "       [1.        ],\n",
       "       [0.        ],\n",
       "       [0.99204749],\n",
       "       [0.32574102]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h((x[i] @ theta))\n",
    "np.array([1])\n",
    "# np.view(x[i], (1, 5))\n",
    "xT = x[i].reshape((x[i].shape[0], 1))\n",
    "xT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[131]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m log_likelihood = y[i] * (theta @ x[i]) - np.exp(theta @ x[i]) - np.log(np.array([\u001b[43mmath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfactorial\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m]))\n",
      "\u001b[31mTypeError\u001b[39m: 'numpy.float64' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "log_likelihood = y[i] * (theta @ x[i]) - np.exp(theta @ x[i]) - np.log(np.array([math.factorial(y[i])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.37861291e+07, 7.98960297e+06, 5.72229559e+06, ...,\n",
       "       1.30367137e+07, 1.61198107e+08, 6.82876138e+06], shape=(2500,))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = y * (x@theta) - h(x@theta) # - np.log(math.factorial(y))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta: [7.24900432 3.87791461 3.37108971 2.04987811 4.56785931]\n",
      "loss: -52196486.40377476\n",
      "theta: [7.35470024 3.9276774  3.42702284 2.00379563 4.41781981]\n",
      "loss: -52198874.8192148\n",
      "theta: [7.36539386 3.93275208 3.43264179 2.00032669 4.40176443]\n",
      "loss: -52198901.20656444\n",
      "theta: [7.36647453 3.93326498 3.43320955 1.99997737 4.40014024]\n",
      "loss: -52198901.51555419\n",
      "theta: [7.36658354 3.93331671 3.43326683 1.9999421  4.39997642]\n",
      "loss: -52198901.522623315\n",
      "theta: [7.36659453 3.93332193 3.4332726  1.99993855 4.3999599 ]\n",
      "loss: -52198901.52309103\n",
      "theta: [7.36659564 3.93332245 3.43327318 1.99993819 4.39995823]\n",
      "loss: -52198901.52313571\n",
      "theta: [7.36659575 3.93332251 3.43327324 1.99993815 4.39995806]\n",
      "loss: -52198901.523140185\n",
      "theta: [7.36659576 3.93332251 3.43327325 1.99993815 4.39995805]\n",
      "loss: -52198901.52314064\n",
      "theta: [7.36659576 3.93332251 3.43327325 1.99993815 4.39995804]\n",
      "loss: -52198901.523140684\n",
      "theta: [7.36659576 3.93332251 3.43327325 1.99993815 4.39995804]\n",
      "loss: -52198901.523140684\n",
      "theta: [7.36659576 3.93332251 3.43327325 1.99993815 4.39995804]\n",
      "loss: -52198901.523140684\n",
      "theta: [7.36659576 3.93332251 3.43327325 1.99993815 4.39995804]\n",
      "loss: -52198901.523140684\n",
      "theta: [7.36659576 3.93332251 3.43327325 1.99993815 4.39995804]\n",
      "loss: -52198901.523140684\n",
      "theta: [7.36659576 3.93332251 3.43327325 1.99993815 4.39995804]\n",
      "loss: -52198901.523140684\n",
      "theta: [7.36659576 3.93332251 3.43327325 1.99993815 4.39995804]\n",
      "loss: -52198901.523140684\n",
      "theta: [7.36659576 3.93332251 3.43327325 1.99993815 4.39995804]\n",
      "loss: -52198901.523140684\n",
      "theta: [7.36659576 3.93332251 3.43327325 1.99993815 4.39995804]\n",
      "loss: -52198901.523140684\n",
      "theta: [7.36659576 3.93332251 3.43327325 1.99993815 4.39995804]\n",
      "loss: -52198901.523140684\n",
      "theta: [7.36659576 3.93332251 3.43327325 1.99993815 4.39995804]\n",
      "loss: -52198901.523140684\n"
     ]
    }
   ],
   "source": [
    "m, n = x.shape\n",
    "theta = np.zeros(n)\n",
    "def h(x):\n",
    "    return np.exp(x)\n",
    "\n",
    "epoch = 20\n",
    "for _ in range(epoch):\n",
    "    for i in range(len(x)):\n",
    "        logits = h(x[i] @ theta)\n",
    "        gradient = x[i].reshape((x[i].shape[0], 1)) @ np.array([y[i] - logits])\n",
    "        theta += 0.00000001 * gradient\n",
    "    rlogits = h(x @ theta)\n",
    "    loss = -(y * np.log(rlogits) - rlogits).mean()\n",
    "    print(f\"theta: {theta}\")    \n",
    "    print(f\"loss: {loss}\")    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-661.84271855,  509.99086539,  463.81740282, ...,  240.31516983,\n",
       "        527.14997129,  613.99668473], shape=(2500,))"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 6\n",
    "logits = h(x @ theta)\n",
    "(y - logits)\n",
    "# theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LWR\n",
    "\n",
    "<img src=\"lwr.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import numpy as np\n",
    "x_train, y_train = util.load_dataset(\"/Users/musazenbilci/Desktop/mosesopposite/cs229-2018-autumn/problem-sets/PS1/data/ds5_train.csv\")\n",
    "x_test, y_test = util.load_dataset(\"/Users/musazenbilci/Desktop/mosesopposite/cs229-2018-autumn/problem-sets/PS1/data/ds5_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2668126980371306\n",
      "0.9939254742242551\n",
      "1.0251723736942564\n",
      "0.1840991956104546\n",
      "1.1046836612534503\n",
      "0.006630152307551651\n",
      "0.15157919663094913\n",
      "1.3847125119859336\n",
      "0.9047927668850024\n",
      "0.9391076559640481\n",
      "0.7902746272191268\n",
      "0.0031967116256148994\n",
      "0.7420577185005341\n",
      "0.4707860505340508\n",
      "0.25901121461666476\n",
      "0.8830621752618671\n",
      "0.0673944647825232\n",
      "0.5455072976014387\n",
      "0.9373208174025089\n",
      "0.4964294761816516\n",
      "0.0951337481537371\n",
      "0.3180855648341377\n",
      "0.20843366496814228\n",
      "0.6932920369293933\n",
      "0.8871902718187191\n",
      "0.758638994754532\n",
      "0.7949801422898414\n",
      "0.34529071761406926\n",
      "0.7963226377115491\n",
      "0.9450685083728148\n",
      "0.24176695705517565\n",
      "0.6065674215143295\n",
      "0.2728120369454359\n",
      "0.157762502787968\n",
      "0.1547442001570197\n",
      "0.29709270518323494\n",
      "0.11302456402638385\n",
      "0.9938068693028574\n",
      "0.2694576651430219\n",
      "0.14438167843645772\n",
      "0.5177646519725173\n",
      "0.20821712783136712\n",
      "0.2663409805347371\n",
      "0.4802418864149762\n",
      "0.4031442411037825\n",
      "0.26503653497532265\n",
      "0.36904473098724033\n",
      "0.12585723549243732\n",
      "0.7905826325068395\n",
      "0.6822001322577501\n",
      "0.1253533228359066\n",
      "0.38063303597723863\n",
      "0.034074358397042405\n",
      "0.12601065929168032\n",
      "0.07459903168433979\n",
      "0.05452466971210024\n",
      "0.0531228305654724\n",
      "0.9908690418050248\n",
      "0.4404585673112863\n",
      "0.48028244140233944\n",
      "0.2919545094457423\n",
      "0.31877736014658575\n",
      "0.8431838793478287\n",
      "0.11953901099107457\n",
      "1.2088804342644688\n",
      "0.700298589786703\n",
      "1.2320297524942518\n",
      "0.25380702239738706\n",
      "0.3763819291316497\n",
      "0.5109126534551627\n",
      "0.6342436350835143\n",
      "0.02883176304709484\n",
      "0.7988012074347594\n",
      "1.0643299691175137\n",
      "0.8896861267302325\n",
      "0.08017628503360319\n",
      "0.035568410073674506\n",
      "0.9192920635825719\n",
      "0.05994230769431222\n",
      "0.12685407680062571\n",
      "0.050894853732775025\n",
      "0.5072877662009665\n",
      "0.9065629452515381\n",
      "0.6168734601497775\n",
      "0.2333853214109085\n",
      "0.9853159558496758\n",
      "0.11536928308015955\n",
      "0.6000130331346103\n",
      "0.5501926125060299\n",
      "0.25828145938224917\n",
      "0.6284493757386197\n",
      "0.4860033388234311\n",
      "0.6322027687526259\n",
      "0.12083068013314381\n",
      "0.6415973025419445\n",
      "0.058405478644629014\n",
      "0.1931034453747992\n",
      "0.6576370920083127\n",
      "1.048328612452791\n",
      "0.2105273576222476\n",
      "0.5633779269047734\n",
      "0.21542470263156446\n",
      "1.3324439246956452\n",
      "0.08834111746986051\n",
      "0.30879312339500253\n",
      "1.1703766402955291\n",
      "0.24553532084233975\n",
      "0.24703766536987537\n",
      "0.7949530369690995\n",
      "0.4549670658502876\n",
      "0.02802010168773285\n",
      "0.9215810535759509\n",
      "0.474599016234184\n",
      "0.04557207606699043\n",
      "0.9788465517744546\n",
      "0.9335709629810717\n",
      "0.06497196715229936\n",
      "0.017510351339401632\n",
      "0.0204834349118286\n",
      "0.8448258596558902\n",
      "0.3037281030196377\n",
      "0.1420895369878876\n",
      "0.07682460512313445\n",
      "0.3592869499464821\n",
      "0.3676524496310618\n",
      "0.3304410799397947\n",
      "0.6295540119048382\n",
      "0.26681906482264195\n",
      "0.6831144556195841\n",
      "0.27147351075006343\n",
      "0.07070082473142825\n",
      "0.14635887681760062\n",
      "0.08444878917452209\n",
      "0.23651978282875677\n",
      "0.33578340813041857\n",
      "0.10766576902033681\n",
      "0.5852528545401117\n",
      "0.6798161589077771\n",
      "1.0039425234982122\n",
      "0.34690812752324357\n",
      "0.6120197471862346\n",
      "0.9936822218207311\n",
      "0.15663611091344054\n",
      "1.0780155106278713\n",
      "0.3784908438460698\n",
      "0.7901045385905731\n",
      "0.06337865428905415\n",
      "0.9531159856648825\n",
      "0.6111759364083157\n",
      "0.3383729276750299\n",
      "0.963436655936913\n",
      "0.6225170271158745\n",
      "0.4370199846730117\n",
      "0.4424386440049323\n",
      "0.8079253972110914\n",
      "0.6870077147995356\n",
      "0.23176697085989273\n",
      "0.1823169087592082\n",
      "0.2927883723296376\n",
      "0.01691231341557986\n",
      "0.11121804547244174\n",
      "0.14008233101996928\n",
      "0.8905281112300318\n",
      "0.1465164826387251\n",
      "0.5330346337454451\n",
      "0.7617642097278472\n",
      "0.4758672260767213\n",
      "0.1139016206182577\n",
      "0.8723527869628345\n",
      "0.5379546340729502\n",
      "0.3963206512461778\n",
      "1.0296228043967384\n",
      "0.7286868393649601\n",
      "0.2577533366921753\n",
      "0.09272775842131076\n",
      "0.5881715519317896\n",
      "0.05644148977474833\n",
      "0.42205463460253795\n",
      "0.07517295203087948\n",
      "0.6704239967889253\n",
      "0.0499036847594625\n",
      "0.42786553905866753\n",
      "0.6440585913721651\n",
      "0.4446254723580699\n",
      "0.18693941749117332\n",
      "0.7041166581742776\n",
      "1.0661700159761063\n",
      "0.3121388955888569\n",
      "0.15418518085184452\n",
      "0.003003050749562697\n",
      "1.366685635344472\n",
      "0.8871408804898372\n",
      "0.24290971008993506\n",
      "1.3204008572226735\n",
      "0.08357767856600118\n",
      "0.07569555823102703\n",
      "0.08746958853888281\n",
      "0.41479940330522946\n",
      "0.5736216137268614\n",
      "0.45005977099937633\n"
     ]
    }
   ],
   "source": [
    "for t in range(len(x_test)):\n",
    "    n = len(x_train)\n",
    "    W = np.zeros((n, n))\n",
    "    T = 0.5\n",
    "    # t = 8\n",
    "    xt = x_test[t]\n",
    "    yt = y_test[t]\n",
    "    y_train = y_train.reshape(-1, 1)\n",
    "    for i in range(n):\n",
    "        W[i][i] = np.exp((-(xt - x_train[i][0])**2) / (2 * T**2)).item()\n",
    "\n",
    "    theta = np.linalg.inv(x_train.T @ W @ x_train) @ (x_train.T @ W @ y_train)\n",
    "    print(np.abs((xt * theta)[0].item() - yt.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300,), (300, 300), (1,))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, W.shape, (x_train.T @ W @ y_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06463599])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = np.linalg.inv(x_train.T @ W @ x_train) @ (x_train.T @ W @ y_train) #.reshape(y_train.shape[0], 1)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.108092528472029), np.float64(0.3749052265091596))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0] @ theta, y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 1), (300, 1))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape, theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.02975001],\n",
       "       [-2.52457661],\n",
       "       [ 2.7946174 ],\n",
       "       [ 1.69590357]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 1), (2,), (2, 2))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0:2].shape, W[0:2].shape, (x_train[0:2] * W[0:2]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300, 1), (300, 1), (300, 1))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = W.reshape(W.shape[0], 1)\n",
    "x_train.shape, W.shape, (x_train*W).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 1),\n",
       " (3, 1),\n",
       " array([[1],\n",
       "        [4],\n",
       "        [9]]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.array([[1], [2], [3]])\n",
    "y1 = np.array([1, 2, 3])\n",
    "y1 = y1.reshape(y1.shape[0], 1)\n",
    "x1.shape, y1.shape,  x1 * y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train * W * x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_model import LinearModel\n",
    "\n",
    "class LocallyWeightedLinearRegression(LinearModel):\n",
    "    \"\"\"Locally Weighted Regression (LWR).\n",
    "\n",
    "    Example usage:\n",
    "        > clf = LocallyWeightedLinearRegression(tau)\n",
    "        > clf.fit(x_train, y_train)\n",
    "        > clf.predict(x_eval)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tau):\n",
    "        super(LocallyWeightedLinearRegression, self).__init__()\n",
    "        self.tau = tau\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \"\"\"Fit LWR by saving the training set.\n",
    "\n",
    "        \"\"\"\n",
    "        # *** START CODE HERE ***\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        # *** END CODE HERE ***\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Make predictions given inputs x.\n",
    "\n",
    "        Args:\n",
    "            x: Inputs of shape (m, n).\n",
    "\n",
    "        Returns:\n",
    "            Outputs of shape (m,).\n",
    "        \"\"\"\n",
    "        # *** START CODE HERE ***\n",
    "        predictions = []\n",
    "        for xt in x:\n",
    "            n = len(self.x)\n",
    "            W = np.zeros((n, n))\n",
    "            for i in range(n):\n",
    "                W[i][i] = np.exp((-(xt - self.x[i])**2) / (2 * self.tau**2)).item()\n",
    "\n",
    "            theta = np.linalg.inv(self.x.T @ W @ self.x) @ (self.x.T @ W @ self.y)\n",
    "            predictions.append(xt * theta)\n",
    "        return np.array(predictions).flatten()\n",
    "        # *** END CODE HERE ***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10809253, -0.02320168,  0.06857   ,  0.24415736, -0.04129074,\n",
       "        0.35886127,  0.18822755, -0.30857419,  0.08016958, -0.01196518,\n",
       "        0.31726153,  0.00320507,  0.02137835,  0.07459675, -0.27168437,\n",
       "       -0.14456169,  0.46229952,  0.09158327,  0.06559322, -0.34874019,\n",
       "        0.07342643, -0.13554041, -0.0194822 , -0.21523376, -0.09733929,\n",
       "       -0.0984498 , -0.02116349, -0.0362843 , -0.08295422, -0.29257002,\n",
       "        0.48106296,  0.46228092, -0.01925451,  0.07597857,  0.09040299,\n",
       "        0.57791596,  0.07947224,  0.06538456, -0.39128041,  0.4202518 ,\n",
       "       -0.02531267, -0.02606352, -0.00825445, -0.07533382, -0.05699588,\n",
       "        0.57797871,  0.42373586,  0.44763394,  0.23185159, -0.07420662,\n",
       "        0.57697982,  0.41978752,  0.01478812,  0.02597826,  0.33216602,\n",
       "        0.29875778,  0.44493812, -0.09198549, -0.02875572,  0.54845131,\n",
       "        0.5387758 , -0.03453282, -0.09521599,  0.07044772, -0.09801627,\n",
       "        0.02751528, -0.30339894, -0.05011201,  0.0865589 ,  0.24108701,\n",
       "       -0.08512857,  0.16127359, -0.09672912, -0.11098947, -0.14037677,\n",
       "        0.40655594,  0.31292824, -0.09879263,  0.27765701,  0.15555024,\n",
       "        0.33512227, -0.25107004,  0.024953  , -0.03153115, -0.00485365,\n",
       "        0.03306271, -0.03936838,  0.08763252,  0.34579494, -0.25930835,\n",
       "       -0.09431454, -0.01253783,  0.32695116,  0.11920045,  0.51472715,\n",
       "        0.08892494, -0.0476592 , -0.09756788,  0.03910416,  0.42042736,\n",
       "       -0.10649387,  0.18184306, -0.30545205,  0.32895254,  0.57426649,\n",
       "        0.04053391, -0.03951959, -0.01912803, -0.13651008, -0.24697908,\n",
       "        0.01034344, -0.09795784,  0.17947177,  0.55901816,  0.04814712,\n",
       "       -0.22289874,  0.40467643,  0.07972088, -0.0278896 , -0.06717445,\n",
       "       -0.02165673,  0.25201548,  0.08605063, -0.26515182,  0.13606064,\n",
       "        0.08043454,  0.07565929,  0.53179383, -0.06762706,  0.52263093,\n",
       "        0.25229569,  0.30389971, -0.18699035,  0.00110001,  0.12380637,\n",
       "        0.29135112, -0.23117589, -0.02187137, -0.11865298,  0.40850163,\n",
       "        0.01914428, -0.08883143,  0.56596434, -0.01460907, -0.03917938,\n",
       "        0.02259464,  0.35606107, -0.05589805, -0.10577858,  0.03349785,\n",
       "        0.03502177,  0.05891227,  0.07317124,  0.02590404,  0.2491466 ,\n",
       "       -0.02438762, -0.35039358,  0.39378765,  0.09107315,  0.30278533,\n",
       "        0.01353321, -0.06616497,  0.08479383,  0.18668519,  0.09176397,\n",
       "       -0.10488624, -0.12672081, -0.34250297,  0.26026084, -0.05031499,\n",
       "       -0.16944604, -0.01277657, -0.00380273,  0.06654117,  0.57308384,\n",
       "       -0.08713638, -0.00523172,  0.42346626, -0.05426637, -0.05006891,\n",
       "        0.14076796, -0.16147249, -0.07993024,  0.4233786 ,  0.0820557 ,\n",
       "       -0.3642822 ,  0.03121775,  0.0342769 , -0.38492079,  0.3199318 ,\n",
       "       -0.31824441, -0.10408186, -0.06420532, -0.31655429,  0.56081851,\n",
       "        0.3668462 ,  0.23428079, -0.09272011, -0.01889016,  0.53829301])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LocallyWeightedLinearRegression(0.5)\n",
    "clf.fit(x_train, y_train)\n",
    "results = clf.predict(x_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
